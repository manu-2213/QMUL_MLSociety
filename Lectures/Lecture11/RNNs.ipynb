{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X matrix:\n",
      "\n",
      "tensor([[1.6009],\n",
      "        [1.0883],\n",
      "        [0.7494]])\n",
      "\n",
      "H matrix:\n",
      "\n",
      "tensor([[-0.6823, -1.4770, -1.1478,  0.9128],\n",
      "        [-0.0069, -0.7481, -2.2240, -2.4576],\n",
      "        [-1.4790, -0.6129, -0.5043, -0.0699]])\n",
      "\n",
      "W_hx matrix:\n",
      "\n",
      "tensor([[-1.2122,  1.0336, -0.1706,  0.0522]])\n",
      "\n",
      "W_hh matrix:\n",
      "\n",
      "tensor([[ 1.2856, -1.0835,  1.0613, -0.8232],\n",
      "        [ 0.2025, -1.9428, -0.5447,  1.4693],\n",
      "        [ 0.6083,  0.5975,  0.6886, -1.3376],\n",
      "        [-0.4515,  0.7201,  0.9322,  0.8357]])\n",
      "\n",
      "\n",
      "The final result is (X*W_hx + H*W_hh):\n",
      "\n",
      "tensor([[-4.2275,  5.2351, -0.1321,  0.7734],\n",
      "        [-1.7229, -0.5130, -3.6079, -0.1157],\n",
      "        [-3.2093,  3.2161, -1.7760,  0.9724]])\n",
      "\n",
      "\n",
      "The final result concatenating is (W*z):\n",
      "\n",
      "tensor([[-4.2275,  5.2351, -0.1321,  0.7734],\n",
      "        [-1.7229, -0.5130, -3.6079, -0.1157],\n",
      "        [-3.2093,  3.2161, -1.7760,  0.9724]])\n",
      "\n",
      "\n",
      "Success! Both results are equal! :)\n"
     ]
    }
   ],
   "source": [
    "# Lets start by showing the concatenation result \n",
    "\n",
    "import torch\n",
    "\n",
    "X, W_xh = torch.randn(3, 1), torch.randn(1, 4)\n",
    "H, W_hh = torch.randn(3, 4), torch.randn(4, 4)\n",
    "\n",
    "print(f\"X matrix:\\n\\n{X}\\n\\nH matrix:\\n\\n{H}\\n\\nW_hx matrix:\\n\\n{W_xh}\\n\\nW_hh matrix:\\n\\n{W_hh}\\n\\n\")\n",
    "\n",
    "result = torch.matmul(X, W_xh) + torch.matmul(H, W_hh)\n",
    "\n",
    "concatenation = torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))\n",
    "\n",
    "print(f\"The final result is (X*W_hx + H*W_hh):\\n\\n{result}\\n\\n\")\n",
    "\n",
    "print(f\"The final result concatenating is (W*z):\\n\\n{concatenation}\\n\\n\")\n",
    "\n",
    "if torch.allclose(result, concatenation, atol=1e-8):\n",
    "    print(\"Success! Both results are equal! :)\")\n",
    "else:\n",
    "    print(\"Tensors are not equal.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***A couple questions...***\n",
    "\n",
    "Why can RNNs express the conditional probability of a token at some time step based on all the previous tokens in the text sequence?\n",
    "\n",
    "What happens to the gradient if you backpropagate through a long sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train this RNN to function as a character-level language model and train it on a corpus consisting of the entire text of H. G. Wellsâ€™ The Time Machine. We start by loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
